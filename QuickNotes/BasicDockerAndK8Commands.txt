//-----------------------------------------------------------------------------------------------------------------
            Basic docker cmd
//-----------------------------------------------------------------------------------------------------------------

>> Pull nginx image
docker pull nginx:latest

>> List all docker images present in system
docker images

>> Run docker image as container. Give name to container 'webserver'
docker run --publish 80:80 --name webserver nginx
Note: use -d to run in detached mode

>> show all running and terminated container
docker ps -a

>> Stop running container
docker stop webserver

>> Kill running container
docker kill webserver

>> remove docker container from local machine (stopped state)
docker rm webserver

docker rm  $(docker ps -a -q)     # Removes all stoped container

docker rmi [imageName]            # Delees the image

docker system prune -a            # Removes all images not in use by any container

>> Docker compose allows users to create yaml file to start multiple service/containers with defined attribute.


docker build -t [name:tag] .     # builds an image using a Dockerfile located in the same folder

docker build -t [name:tag] -f [fileName]     # builds an image using a Dockerfile located in different folder

docker tag [imageName] [name:tag]            # Tag an existing image


//-----------------------------------------------------------------------------------------------------------------
            Volume commands
//-----------------------------------------------------------------------------------------------------------------
docker create volume [volume name]                      # Creaes a new volume

docker volume ls                                        # List the volumes

docker volume inspect [Volume Name]                     # Displays the volume info

docker volume rm [Volume Name]                          # Deletes a volume

docker volume prune                                     # Delete all volume not mountedd



//-----------------------------------------------------------------------------------------------------------------
            YAML
//-----------------------------------------------------------------------------------------------------------------
YAML : YAML Ain't Markup Language
Seialization standard and deafult for docker

YAML Lint to parse and raise warring if there is any issue


//-----------------------------------------------------------------------------------------------------------------
            Docker compose
//-----------------------------------------------------------------------------------------------------------------
Define and run multi container application
Define using YAML files
Run using the dokcer CLI with the compose plugin (Docker compse)

docker compose vs docker-compse: 
    - docker-compse was Compose V1  wich uses python in backend
    - dokcer compose is introduce in Compose V2 (DockerCon Live 2022). Faster version
        - Installed with Docker Desktop
            - Linux: apt-get install docker compose-plugin

-- Docker compose commands

docker compose build                                # Build the images

docker compose start                                # Start the containers

docker compose stop                                 # stop the containers

docker compose up -d                                # Build and start

docker compose ps                                   # List what's running

docker compose rm                                   # Remove from memory

docker compose down                                 # Stop and remove

docker compose log                                  # Get the logs

docker compose exec [container] bash                # Run a command in a conainer

Note: Once docker compose command run, we can not create another container as earlier instance already have containers running but with Compose V2 its possible.
//------ Compose V2 commands

docker compose --project-name test1 up -d           # Run an instance as a projet

docker compose -p test2 up -d                       # shortcut of above command

docker compose compose ls                           # List running projects

docker compose cp [containerID]:[SRC_PATH] [DEST_PATH] # copy files from the containers

docker compose cp [SRC_PATH] [containerID]:[DEST_PATH] # copy files to the containers


_________________________________________________________________________________________________________________________________________________________________________________________________
//-----------------------------------------------------------------------------------------------------------------
            Kubernetes a.k.a. K8s
//-----------------------------------------------------------------------------------------------------------------
Originated from Google
v1.0 releaseed on July 2015
3rd gen container schduler from Google
   - Previously Google internal project: Bor and Omega
Donated to the Cloud Native Computing Foundation (CNCF): www.cncf.io

What is Kubernetes?
- K8x is the leading continer orchestration tool
- Designed as a looly coupled collection of componentes centered around deploying, maintainig and scalin workloads
- Vendor nutral

What K8s can do?
- Service discovery and load balancing
- Storage orchestration
  - Loal or Cloud based
- Automated rollouts and rollbacks
- self healing
- secre and configuratio management
- use the same API across on-premise and every cloud providers

What K8s can't do?
- Does not deploy source code
- Does not build your application
- Does not provide application-level servies
  - Mesage buses, databases, caches etc


//-----------------------------------------------------------------------------------------------------------------
            K8s Resources
//-----------------------------------------------------------------------------------------------------------------
 https://kubernetes.io/docs/concepts/overview/
 
Local K8s (Requires Virtualization)
- Docker Desktop
    - Limited to 1 Node
- MicroK8s (From Ubuntu)
    - Multiple Nodes
- Minikube
    -  Multiple Nodes
- 'Kind' :runs over Docker Desktop and provide extra functionality
    - Multiple Nodes

Note: Minikube can be installed on Hyper-V or VirtualBox


Minikuvbe Hyper-V Installation:
- Run windows terminal or powershell in admin mode
- Install Chocolatey package manager
- Run: choco install minikube
- Create network switch
    - New-VMSwitch -Name "MinikubeNAT" -AllowManagement $True - NetAdapterName "<adapter_name>"
- Start Minikube
    - minikuber start --vm-driver hyperv --hperv-virtual-switch "MinikubeNAT"
- Switch context
    - kubectl config use-context minikube


//-----------------------------------------------------------------------------------------------------------------
            kubectl aka CLI and Context
//-----------------------------------------------------------------------------------------------------------------
kubectl (KUBE C-T-L or KUBE KUTTLE or Kube Control) is Kubernetes CLI
Communicates with teh apiserver
configuration stored locally:
    - ${HOME}/.kube/config
    - C:\Users\{USER}\.kube\config

K8s Context:
--------------------------
- A context is a group of access parameters to a K8s cluster
- Contains a Kubernetes cluster, a user, and a namespace
- The current context is the cluster that is curently the default for kubectl
    - All kubctl commands run against that cluster


kubectl config current-context                           # Get the curent context
Kubectl config get-contexts                              # List all context
Kubectl config use-context [contextName]                 # Set the current context
Kubectl config delete-context [contextName]              # Delete a context from the config fle

//-----------------------------------------------------------------------------------------------------------------
            Imperative way vs Declarative way in Kubernetes
//-----------------------------------------------------------------------------------------------------------------
Imperative:
  - Using kubectl commands, issue a series of commands to create esources 
  - Great for learning, testing and troubleshooting
  - It's like code
Declarative:
  - Using kubectl and YAML mainfests defining the resources that you need
  - Reproducible, repeatable
  - Can be saved in source control
  - It's like datat that can be parsed and modified



## Imperative

    kubectl create deployment mynginx1 --image=nginx

## Declarative

    kubectl create -f deploy-example.yaml

## Cleanup

    kubectl delete deployment mynginx1
    kubectl delete deploy mynginx2

kubectl get deploy                                     # To get list of deployment


** Note: Every .yaml file contains 'kind:' tag which determines underlying subset of kubectl functionality. Hence command 'kubectl create -f <.yaml>' would be repeated accross various topics
//-----------------------------------------------------------------------------------------------------------------
            Namespaces
//-----------------------------------------------------------------------------------------------------------------
- Allow to group resources. Ex. Dev, Test, Prod
- K8s creates a deafult workspace: default
- Objects in one namespace can access object in a different namespace.
    - Ex: objectname.prod.svc.cluster.local
- Deleting a namespace will delete all its child object


kubectl get namespace                                  # List all namespace
kubectl get ns                                         # List all namespace: Shortcut

kubectl config set-context --curent --namespace=[namespaceName]  # set the current contectto use a namespace

kubectl create ns [namespaceName]                      # Create a namespace

kubectl delete ns [namespaceName]                      # Delete a namespace

kubectl get pods --all-namespaces                      # List all pods in all namespace


//-----------------------------------------------------------------------------------------------------------------
            Kubernetes components
//-----------------------------------------------------------------------------------------------------------------
---------Master node----------

kube-apiserver
- REST Interface
- Save state to the datastor (etcd)
- All clients interact with it, never directly to the datastore


etcd
- Act as the cluster datastore for storing state
- Key-value stored
- Not a database or a datastore for application to use
- The single source of truth



kube-control-manager
- The controller of controllers!
- It runs controllers
  - Node controller
  - Replicate controller
  - Endpoints controller
  - Service account & Token controllers


cloud-control-manager
- Interact with the cloud providers controllers
  - Node
     - For cvhecking the cloud provider to determine if a node has been deleted in cloud after it stops responding
  - Route
     - For setting up routes in ther underlying cloud infrastruture
  - Service
     - For creating, updating and deleting cloud provider load balancer
  - Volume
     - For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.


Kube-scheduler
- Watches newly created pods that have no node assigned, and selects a node for them to run on
- Factors taken into  account for scheduling decisions include
  - individual and collective resource requirments
  - Hardware/software/policy constraints
  - Affinity and anti-affinity specifications
  - Data locality


Addons
- DNS
- Web UI (Dashboard)
- Cluster-level logging
- Container resource monitoring

---------Worker node----------

kube-proxy
- A network proxy
- Manages network rules on nodes

kubelet
- Manag the pods lifecycle
- Ensure that the container described in the Pod specs are running and healthy

container runtime
- K8s support several container runtimes
- Must implement the kubernetes Container Runtime Interface
  - Moby
  - Containerd
  - Cri-O
  - Rkt
  - Kata
  - Virtlet


Pods
- Atomic unit of the smallest unit of work of K8s
- Encapsulates an application's container
- Represents a unit of deployments
- Pods can run one or multiple container
- Container within a pod share
  - IP address space, mounted volumes
- Containers within a pod can communicate via
  - Localhost, IPC
- Pods are ephemeral
- Deploying a po is an atomic operation, it succeed or not
- If a pod fails, it is replaced with a new one with a shiny new IP address
- You don't update a pod, you replace it with an updated version
- You scale by adding more pods, not more containers in a pod


Pod Lifecycle:
Pod States are:
- Pending
  - Accepted but not yet created
- Running
  - Bound to a node
- Succeeded
  - Exited with Status 0
- Failed
  - All containers exit and at least  oe exited with non-zero status
- Unknow
  - Communication issues with the pod
- CrashLoopBackOff
  - Started, crashed, started again, and then creashed again
  
//-----------------------------------------------------------------------------------------------------------------
            Kubectl Pod commands
//-----------------------------------------------------------------------------------------------------------------

kubectl create -f [pod-definition.yml]                           # Create a pod

kubectl run [podname] --image=busybox -- /bin/sh -c "sleed 3600"     # Run a pod-definition

kubectl get pods                                                 # List the running pods

kubectl get pods -o wide                                         # Same but with more info

kubectl describe pod [Podname]                                   # Show pod info

kubectl get pood [podname] -o yaml > file.yaml                   # Extract the pod definition in YAML and save it to a file

kubectl exec -it [podname] -- sh                                 # Interactive mode

kubectl delete -f [pod-definition.yml]                           # delete a pod

kubectl delete pop [podname]                                     # same using the pod's name



Init containers: Runs inside pod before running application container. 
E.g. init container can run and check if database is up and running before running actual application container

- Alwayas runs to completion
- Each init container must complete successfully before the next one starts
- If ifails, the kubelet repeatedly restarts it until it succeeds
  - Unless it's restartPolicy is set to Never
- Probes are not supported
  - livenessProbe, readinessProbe, or startupProbe


//-----------------------------------------------------------------------------------------------------------------
            Selectors
//-----------------------------------------------------------------------------------------------------------------


Labels : Key-value pairs used to identify, describe and group related sets of objects or reources. These pairs are user defined.
Example:
  labels:
    app: myapp
    type: front-end


Refer: K8s-Fundamentals-HandsOn\L22-02 Selectors for exposing container as service on localhost


//-----------------------------------------------------------------------------------------------------------------
            Multi-containner Pods
//-----------------------------------------------------------------------------------------------------------------

Typical patterns - Sidecar
---------------------------
Container within Pod
  1. App: focuses on core buisness logic. E.g. it write some logs to files.
  2. Sidecar: Copy the log files to peristent storage (eg. storage provided by cloud provider)

Typical patterns - Adaptor
---------------------------

Typical patterns - Ambassador
---------------------------

Book: O'reilly  Designing Distrubuted Systems Patterns and paradigms for scalable, reliable services (Berndan Burns)


kubectl create -f [pod-definition.yml]                             # Create a pod

kubectl exec -it [podname] -c [containername] -- sh                # Exec into a pod

kubectl logs [podname] -c [containername]                          # Get the loogs for a container


---------------------
Networking Concepts:
---------------------
- All containers within a pod can communicate with each other (using local host by differnet port number)
- All pods can communicate with each other: Using service
- All nodes can communicate with all pods
- Pods are given an IP address (ephemeral i.e. lasting for a very short time)
- **Serices are given a persistent IP**
* External



//-----------------------------------------------------------------------------------------------------------------
            Workloads
//-----------------------------------------------------------------------------------------------------------------
A workload is an application running on Kubernetes
  - Pod
     - Represents a set of running containers
  - ReplicaSet
     - Primary method of managing pod replicas and theirlifecycle to privde set of self healing capability
  - Deployment
  - StatefulSet
  - Deamonset
    - Provide node-local facilities, such as storage driver or network plugin
  - Tasks that run to completion
    - Job
    - CronJob

ReplicaSet
-----------
  - Primary method of managing pod replicas and theirlifecycle to privde set of self healing capability
  - Their job is to always ensure the desired number of pods are running
  - While you can create ReplicaSets, the recommended way is to create Deployments


kubectl apply -f [definition.yaml]                             # Create a ReplicaSet

kubectl get rs                                                 # List ReplicaSets

kubectl describe rs [rsName]                                   # Get info

kubectl delete -f [definition.yaml]                            # Delete a ReplicaSet

kubectl delete rs [rsName]                                     # Same as above but using ReplicaSet name

Note: Using Deployment is better than ReplicaSet


Deployment
----------
- A deployment manage a single pod template
- You create a deployment for each microservice
  - front-end
  - back-end
  - image-procesor
  - creditcard-processor
- Deployments create ReplicaSets in the background
- Don't interact witht the ReplicaSets directly

* ReplicaSet provides self-healing and scalablility functionality *
* Deployment provides rolling updates and rollbacks functionality *


kubectl create deploy [deploymentName] --image=busybox --replicas=3 --port=80        # The imperative way

kubectl apply -f [definition.yaml]                             # Create a deployment

kubectl get deploy                                             # List deployments

kubectl describe deploy [deploymentName]                       # Get info

kubectl get rs                                                 # List ReplicaSets

kubectl delete -f [definition.yaml]                            # Delete a deployment

kubectl delete deploy [deploymentName]                         # Same but using the deployment name


DaemonSet
----------
- Ensures all Nodes (or a subset) run an instance of a Pod.
- Scheduled by the scheduler controller and run by the daemon controller.
- As nodes are added to the cluster Pods are added to them
- Typical uses
  - Running a cluster storage daemon.
  - Runing a logs collection daemon on every node.
  - Running a node monitoring on every node.

kubectl apply -f [definition.yaml]                             # Create a DaemonSet

kubectl get ds                                                 # List DaemonSet

kubectl describe ds [dsName]                                   # Get info

kubectl delete -f [definition.yaml]                            # Delete a DaemonSet

kubectl delete rs [dsName]                                     # Same as above but using DaemonSet name




StatefulSet
----------
- For Pods that must persist or maintain state
- Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods
- Each has a persistent identifier (name-x)
- If a pod dies, it is replced with another one using the idetifier
- Creates a series of pods in sequence from 0 to X and deletes them from X to 0.
- Typical uses:
  - Stable, unique network identifiers
  - Stable, databases using persistent storage

Warrnings:
- Containers are stateless by design
- StatefulSets offers a solution for stateful scenarios
- A better approach could be to use the Cloud provider database services
- Deleting a StatefulSet will not delete the PVCs
  - You have to do this manually.

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).

What is difference between PV and PVC?
A persistent volume (PV) is a piece of storage in the Kubernetes cluster, while a persistent volume claim (PVC) is a request for storage. 


kubectl apply -f [definition.yaml]                             # Create a StatefulSet

kubectl get sts                                                # List StatefulSet

kubectl describe sts [stsName]                                 # Get info

kubectl delete -f [definition.yaml]                            # Delete a StatefulSet

kubectl delete sts [stsName]                         # Same but using the StatefulSet name




Job
----------
- Workload for short lived tasks
- Creates one or more Pods and ensures that a specified number of them successfully terminate
- As pods successfully complete, the Job tracks the successful completions
- When  specified number of successful completions is reached, the Job complets
- By default, jobs with more than 1 pod will create them one after the other. To create them at the same time, add parallelism.

Note: restartPolicy should be set to Never as default is always.

kubectl create job [jobName] --image=busybox                   # The imperative way

kubectl apply -f [definition.yaml]                             # Create a Job

kubectl get job                                                # List Job

kubectl describe job [jobName]                                 # Get info

kubectl delete -f [definition.yaml]                            # Delete a Job

kubectl delete job [jobName]                                   # Same but using the Job name


CronJob
----------
- An extesion of the Job
- Provides a method of executing jobs on a cron-like scheduler
- UTC only
* * * * *
refer: https://en.wikipedia.org/wiki/CronJob

How to know if cronJob is successful? Using history to view logs of few pods.
- History
  - The last 3 successful jobs are kept (by default)
  - The last failed job is kept
  - Setting successfulJobHistoryLimit to zero keeps none
  
kubectl create cronjob [jobName] --image=busybox --schedule="*/1 * * * *" -- bin/sh -c "date;"                   # The imperative way

kubectl apply -f [definition.yaml]                            # Create a CronJob

kubectl get cj                                                # List CronJob alterate command: kubectl get cronjobs

kubectl describe cj [jobName]                                 # Get info

kubectl delete -f [definition.yaml]                           # Delete a CronJob

kubectl delete cj [jobName]                                   # Same but using the CronJob name


//-----------------------------------------------------------------------------------------------------------------
            Updates
//-----------------------------------------------------------------------------------------------------------------

Strategy: 
1. Recreate
  - Delete existing pods and create new pods
  - Hence service may not be available for small amount of time
2. RollingUpdate
  - maxSurge
    - Maximum number  of Pods that can be created over the desired number of Pods
	- Value or percentage
  - maxUnavailable
    - Maximum number of Pods that can be unavailable during the update processor
  - Default strategy with maxSurge and maxUnavailable both set to 25%

kubectl apply -f [definition.yaml]                            # Update a deployment

kubectl rollout status                                        # Get the progress of the update

kubectl rollout history deployment [deploymentName]           # Get the history of the deployment

kubectl rollout undo [deploymentName]                         # Rollback a deployment

kubectl rollout undo [deploymentName] --to-revision=[revision#]        # Rollback to a revision number



Blue-Green Deployments
-----------------------
While deploying newer version of application via rollout method it may happen the newer version of app uses new db schema and which result in failure.

Blue: What's in production
Green: Newer version deployed but not yet in production

Below diagram shows how Blue Green deployment work
1.

            master node (V1) [Blue]
				 |
			 ____|_____
			/          \
    (v1)[Blue]   (v1)[Blue]        (V2)[Green]   (V2)[Green]

2.
            master node (V2) [Blue]
				 |_______________________________
											 ____|_____
											/          \
    (v1)[Green]   (v1)[Green]        (V2)[Blue]   (V2)[Blue]

Note: 
- Above deplyment strategy still does not solve above discussed new db schema problem entirly.
- We need to provision cluster size
Refer "L25-04 Blue-Green Deployments" for quick info


//-----------------------------------------------------------------------------------------------------------------
            Services
//-----------------------------------------------------------------------------------------------------------------

ClusterIP
---------
- ClusterIP is the default service
- Visibility
  - Cluster internal
- Port: The port the service is listening to
- TargetPort: Redirecting to the port the pod(s) are listening to
- Load balanced using round robin
  - Session affinity is configurable
- When to use
  - To provide a durable way to communicate with pods inside the cluster

# Service Defination example
apiVersion: v1
kind: Service
metadata:
 name: svc-front
spec:
  ports:
  - port: 8080
    targetPort: 8080                 # Due to this cotainer port spec should also use same port. i.e. - containerPort: 8080
  selector:
    app: hello-v1


kubectl expose po [podName] --port=80 --target-port=8080 --name=frontend                   # Create a service to expose a pod

kubectl expose deploy [deployName] --port=80 --target-port=8080                            # Create a service to expose a pod

kubectl apply -f [definition.yaml]                            # Deploy service

kubectl get svc                                               # Get service list

kubectl get svc -o wide                                       # Get extra info

kubectl describe svc [serviceName]                            # Describe the service

kubectl delete -f [definition.yaml]                           # Delete the service

kubectl delete svc [serviceName]                              # Delete the serviceusing the serviceName name



NodePort
---------
- NodePort extend the ClusterIP service
- Visibility: Internal and External
- NodePort:
  - The port the service is listening to externally
  - Statically defined, or dynamically taken from a range between 30000-32767
- Port: The port the service is listening to internally
- Nodes must have public IP addresses
- Use any Node IP + nodePort to access the service


kubectl expose po [podName] --port=80 --target-port=8080 --type=NodePort                                 # Create a service to expose a pod

kubectl expose deploy [deployName] --port=80 --target-port=8080 --type=NodePort --name=frontend          # Create a service to expose a pod

kubectl apply -f [definition.yaml]                            # Deploy service

kubectl get svc                                               # Get service list

kubectl get svc -o wide                                       # Get extra info

kubectl describe svc [serviceName]                            # Describe the service

kubectl delete -f [definition.yaml]                           # Delete the service

kubectl delete svc [serviceName]                              # Delete the serviceusing the serviceName name



Services
---------
- A service is another type of K8s object
- Pod IPs are unreliable but service Ips are
- Durable (unlike pods)
  - Static IP address
  - Static DNS name
  - [servicename].[namespace].svc.cluster.local
- Services are ways to access pods
- Target pods using selectors


Services are of below types:
- ClusterIP: Visibility internal
- NodePort: Exposes node to externally
- LoadBalancer (L4)
- Ingress (L7)

Load Balancing
------------------
- Layer 4 Load Balancing
  - Operating at the transport level (TCP)
  - Unable to make decisions based on content
  - Simple algorithms such as round-robin routing
- Layer 7 Load Balancing
  - Operates at the application level (HTTP, SMTP, etc)
  - Able to make decisions based on the actual content of each message
  - More intelligent load balancing deisions and content optimizations
    - Routing rules



//-----------------------------------------------------------------------------------------------------------------
            Storage & Persistence
//-----------------------------------------------------------------------------------------------------------------
- We need to store data outside the container in a Volume
- Volumes let containers store data into external storage systems
- Vendors ceate plugins for their storage sstems according to the Container Storage Interface
- Two ways to create storage: Static and Dynamic

1. Persistent Volume 
2. StorageClass

Storage - Static way
---------------------

-----------------------------
Persistent Volumes and Claims
-----------------------------
- Persistent Volumes 
  - Represent a storage resource
  - Cluster wide
  - Provisioned by an administrator
- Persistent Volume Clain
  - A one to one mapping to a persistent volume
- One or more pods can use a Persistent Volume Claims
- Can be consumd by any of ther containers within the pod

Below are few of the popular PV plugins
GCEPersistentDisk
AWSElasticBlockStore
AzurFile
CSI
FC (Fibre Channel)
FlexVolume
Flocker
NFS
iSCSI
RBD (Ceph Block Device)
CephFS
Cinder (OpenStackblock storage)
Glusterfs
VsphereVolume
Quobyte Volumes
Portworx Volumes
ScalelO Volumes
StorageOS
*HostPath*: Single Node testing only -- Local storage is not supported in any way and will not work in a multi-node cluster

Reclaim Policies:
------------------
- Delete (Default)
  - Deletes the data upon pod deletion
- Retain
  - Keeps the data upon pods deletion

Access Mode
------------------
- ReadWriteMany
  - The volume can be mounted as read-write by many pods
- ReadOnlyMany
  - The volume can be mounted read-only by many pods
- ReadWriteOnce
  - The volume can be mounted as read-write by a single pod
  - The other pods are in read only mode
  - The one that has mounted the volume first will be able to write


Persistent Volume States
------------------------
- Aavailable
  - A free resource that is not yet bound to a claim
- Bound
  - The volume is bound to a claim
- Released
  - The claim has been deleted, but the resource is not yet relclaimed by the cluster
- Failed
  - The volume has failed its automatic reclaimation



kubectl apply -f [definition.yaml]                            # Deploy the PVs and PVCs

kubectl get pv                                                # Get the PV list

kubectl get pvc                                               # Get the PVC list

kubectl describe pv [pvName]                                  # Describe the PV

kubectl describe pvc [pvcName]                                # Describe the PVC

kubectl delete -f [definition.yaml]                           # Delete the PVs and PVCs

kubectl delete pv [pvName]                                    # Delete the pv using it's name

kubectl delete pvc [pvcName]                                  # Delete the pvc using it's name


-------------
StorageClass
-------------
- Describes the "classes" of storage offered by the admin
- An abstraction on top of an external storage resource
- No need to set a capacity
- Eliminates the need for the admin to pre-provision a persistent volume 

Reclaim Policies:
------------------
- Delete (Default)
  - Deletes the data upon pod deletion
- Retain
  - Keeps the data upon pods deletion

Access Mode
------------------
- ReadWriteMany
  - The volume can be mounted as read-write by many pods
- ReadOnlyMany
  - The volume can be mounted read-only by many pods
- ReadWriteOnce
  - The volume can be mounted as read-write by a single pod
  - The other pods are in read only mode
  - The one that has mounted the volume first will be able to write


kubectl apply -f [definition.yaml]                            # Deploy the StorageClass and PVC

kubectl get sc                                                # Get the StorageClass list

kubectl get pvc                                               # Get the PVC list

kubectl describe sc [className]                               # Describe the StorageClass

kubectl delete -f [definition.yaml]                           # Delete the StorageClass and PVC

kubectl delete sc [className]                                 # Delete the StorageClass using it's name

kubectl delete pvc [pvcName]                                  # Delete the pvc using it's name




//-----------------------------------------------------------------------------------------------------------------
            Application Settings
//-----------------------------------------------------------------------------------------------------------------
Like in docker, we can use environment variable to influence the working of container, in K8s we can achieve same using object like ConfigMaps.


ConfigMaps
-----------
- Decouple and externalize configuration
- Referenced as environment variables
- Created from
  - Manifests
  - Files
  - Directories (containing one or mor files)
- Static meaning that if you change values, the containers will have to be restarted to get them

ConfigMaps and Volumes
-----------------------
- Solves the static issue mentioned above
- Updates to values are refelected in containers
- Each key/Value pair is seen as a file in the mounted directory.



kubectl create configmap literal-example --from-literal="city=Ann Arbor" --from-literal=state=Michigan       # The imperative way

kubectl apply -f [cf.yaml]                                    # The declarative way

kubectl create cm [ame] --from-file=myconfig.txt              # From a file

kubectl create cm [ame] --from-file=config/                   # From a folder

kubectl get cm                                                # List the ConfigMaps

kubectl get cm [name] -o YAML                                 # Save the ConfigMaps in a YAML file

kubectl delete -f [cf.yaml]                                   # Delete a ConfigMap



-----------
Secrets
-----------
Various types of Secrets. Refer Screenshot for more info.

Opaque : Arbitary user-defined data. Similar to ConfigMaps


- Stored as base64 encoded strings
- Not secure as ase64 strings are not encrypted

- Should you use secrets? Depends upone the use cases. More sensetive data should be avoided.
- Protect them using RBAC authorization policies
- Store secrets elsewhere
  - Cloud providers offer ways to secure these secrets
     - Azure Key Vault
	 - AWS Key Management Service
	 - Google Clou KMS
  - HarshiCorp Vault: https://www.vaultproject.io


kubectl create secret generic [secretName] --from-literal=STATE=Michigan       # The imperative way

kubectl apply -f [secret.yaml]                                      # The declarative way

kubectl get secrets                                                 # List the Secrets

kubectl get secrets [secretName] -o YAML                            # Save a Secret in a YAML file

kubectl delete -f [secret.yaml]                                     # Delete a secret

kubectl delete secrets [secretName]                                 # Delete a secret



//-----------------------------------------------------------------------------------------------------------------
            Obervability
//-----------------------------------------------------------------------------------------------------------------

Probes
---------
- Startup probes
  - To know when a container has started
- Readiness Probes
  - To know when a container is ready to accept traffic
  - A failing readiness probe will stop the application from receiving traffic
- Liveness Probes
  - Indicates whether the code is running or not
  - A failing Liveness Probes will restart the container

Probing the container:
- The kubelet checks periodically the containers using probes
- ExecAction
  - Execute a command inside the container
- TCPSocketAction
  - Check if a TCP socket port is open
- HTTPGetAction
  - Performs an HTP GET against a specific port and path


//-----------------------------------------------------------------------------------------------------------------
            Dashboards
//-----------------------------------------------------------------------------------------------------------------

Kubernetes CLI:
kubectl create -f deploy.yaml
kubectl get pods -o wide
kubectl top nodes
kubectl logs mypod


3 dashboards:
-------------
- K8s
- Lens
- K9s

Dashboard - Web UI
-------------------
- Kubernetes offers a Web UI
- Not installed by default by
  - Docker Desktop
  - AWS EKS
  - Azure AKS
  - GCP GKE
  - Linode LKE
- Don't install it if not needed: Its inernal and hence need to be expsed to be accessible. Can be used as attack vector


Lens
--------
- Kubernetes IDE
- Runs locally
  - Mac, Windows, Linux
- Built-in terminal
- Maintained by Mirantis
  - htttps://k8slens.dev
  
  
Dashboard- Lens
-----------------
- Dashboard in a terminal!
- Windows: choco install k9s
- Mac: brew install k9s
- Linux: https:github.com/derailed/k9s





